{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ipvlz_kJ2hex"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVD9Qql_-vEP",
        "outputId": "90a8eff2-5596-44cf-a14c-7d0cfd804b33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "cLx43OnS-RNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "O_A_vQVt7nz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip uninstall tokenizers\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "sYHyNGzi7s_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce commodo mauris id\""
      ],
      "metadata": {
        "id": "1PymmBWN_3dE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdQ1_2P3ABHY",
        "outputId": "e8c1717a-1d14-4025-bf06-6e392ac03750"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMIblg0PAINf",
        "outputId": "090c2ef2-0882-4ac2-f3ba-34f133367e8e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.', 'Fusce commodo mauris id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_tag=(word_tokenize(text))\n",
        "print(pos_tag(to_tag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4pUOp6qAMYp",
        "outputId": "7bd2a33b-86d5-4289-ed33-24a0798ee041"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Lorem', 'NNP'), ('ipsum', 'NN'), ('dolor', 'NN'), ('sit', 'NN'), ('amet', 'NN'), (',', ','), ('consectetur', 'NN'), ('adipiscing', 'VBG'), ('elit', 'NN'), ('.', '.'), ('Fusce', 'NNP'), ('commodo', 'JJ'), ('mauris', 'NN'), ('id', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXCXx8YhAge9",
        "outputId": "f71b933b-399c-4159-b891-be71431eb72b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'in', \"that'll\", 'into', 'both', 'up', 'these', 'only', 'shouldn', \"mightn't\", 'down', \"aren't\", 'herself', 'having', 'does', 'by', 'most', 'that', 'out', 'under', 'won', 'for', 'about', 'ain', 'can', \"couldn't\", 'a', 'they', \"won't\", 'through', 'shan', 'no', 'some', \"hadn't\", 'd', 'didn', 'is', 'hasn', 'themselves', 'few', 'from', 'who', 'as', \"hasn't\", 'doing', 'other', 'and', 'same', 'me', 'once', 't', 'she', 'he', 'after', 'ma', 'whom', 'itself', 'before', 'all', 'just', 'an', 'myself', 'above', 'being', 're', 'here', 'o', 'did', 'your', 'have', \"shan't\", \"you've\", 'wasn', \"it's\", 'during', 'on', 'this', \"should've\", 'mightn', \"wouldn't\", 'yours', 'so', 'doesn', \"weren't\", 'him', 'which', 'own', \"mustn't\", 'll', 'needn', 'off', 'i', 'its', 'himself', 'until', 'there', \"don't\", 'had', \"needn't\", 'yourselves', 'against', 'y', 'we', 'not', 'has', 'to', 'with', 'further', 'of', 'over', 'or', 'between', \"you're\", 'couldn', 'hers', 'his', 'should', 'them', \"isn't\", \"shouldn't\", \"you'd\", 'yourself', 'how', \"wasn't\", 'now', 've', 's', 'any', 'isn', 'their', 'more', 'nor', 'what', \"didn't\", 'don', 'you', 'the', 'ourselves', 'such', 'it', 'while', 'weren', 'when', 'will', 'hadn', 'because', 'why', \"she's\", 'are', 'too', 'than', 'm', 'wouldn', 'ours', 'her', 'was', \"you'll\", 'again', 'where', 'theirs', 'haven', 'am', 'be', 'but', 'then', 'at', 'my', 'been', \"doesn't\", 'each', 'below', \"haven't\", 'our', 'aren', 'do', 'mustn', 'were', 'very', 'if', 'those'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_clean = word_tokenize(text)\n",
        "no_stopwords_text = []\n",
        "for token in to_clean:\n",
        "    if(token not in stop_words):\n",
        "        no_stopwords_text.append(token)\n",
        "\n",
        "print(no_stopwords_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2LdKQTZAlOR",
        "outputId": "787fb25d-5b2e-4de2-d42e-f1f4c6f152ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = []\n",
        "for token in no_stopwords_text:\n",
        "    stemmed_word = stemmer.stem(token)\n",
        "    stemmed_words.append(stemmed_word)\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERLI-WbgAtVj",
        "outputId": "1a73ceef-768b-4f75-b6a3-964e8e915462"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipisc', 'elit', '.', 'fusc', 'commodo', 'mauri', 'id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = []\n",
        "for token in no_stopwords_text:\n",
        "    lemmatized = lemmatizer.lemmatize(token)  # Assuming you want to lemmatize verbs (you can change the 'pos' argument as needed)\n",
        "    lemmatized_words.append(lemmatized)\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "1KYoEdmBA3Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "corpus = [\n",
        "    \"I love to eat pizza\",\n",
        "    \"Pizza is my favorite food\",\n",
        "    \"I enjoy eating pizza with friends\",\n",
        "    \"I like to have pizza for dinner\",\n",
        "    \"Pizza toppings include cheese, pepperoni, and mushrooms\"\n",
        "]\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW9c1NeCBBwR",
        "outputId": "2fd384c2-b5c8-4cca-ce2e-4304ef956572"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.58946308 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.58946308 0.         0.         0.\n",
            "  0.28088232 0.4755751  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.48638585 0.48638585 0.         0.         0.         0.\n",
            "  0.48638585 0.         0.         0.         0.48638585 0.\n",
            "  0.23176546 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.48638585 0.48638585\n",
            "  0.         0.         0.         0.48638585 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23176546 0.         0.         0.48638585]\n",
            " [0.         0.         0.45277275 0.         0.         0.\n",
            "  0.         0.         0.45277275 0.         0.45277275 0.\n",
            "  0.         0.45277275 0.         0.         0.         0.\n",
            "  0.21574864 0.36529421 0.         0.        ]\n",
            " [0.40073619 0.40073619 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.40073619\n",
            "  0.         0.         0.         0.40073619 0.         0.40073619\n",
            "  0.19095294 0.         0.40073619 0.        ]]\n",
            "['and' 'cheese' 'dinner' 'eat' 'eating' 'enjoy' 'favorite' 'food' 'for'\n",
            " 'friends' 'have' 'include' 'is' 'like' 'love' 'mushrooms' 'my'\n",
            " 'pepperoni' 'pizza' 'to' 'toppings' 'with']\n"
          ]
        }
      ]
    }
  ]
}